// SPDX-License-Identifier: GPL-2.0-or-later
/*
 * Copyright (C) 2018 Verisilicon Inc.
 *
 * Support manage two slices memory;
 * the memory is in ep side, so the address is in ep pcie axi master view
 * memory space. Every slice have 2GB DDR memory,
 * but low 64MB have same address mapping with Tcache, and high 256MB
 * will be used by DDR ECC, so the valid size that codec can use is 1728MB.
 * One slice memory will be divide into 16 blocks with same size(108MB),
 * and one block contain 27684 chunks, one chunk is 4KB. Block is the
 * minimum unit for video task, every block is only used by a task, so
 * one task will take one block at one time. Next allocation, task will
 * get free memory form itself block firstly, if no, it will get a new
 * block. When a block is not used, it will be freed. One block canâ€™t
 * shared by different tasks.
 * When allocate memory from a block, it also check the first chunk(chunk
 * index is 0), if not be used, reserve it, and record reserved chunk
 * count in the chunk 0, so consecutive chunks will be reserved for this
 * allocation; next allocation, check chunk 0, it is used, get the reserved
 * chunk size(N), then check the chunk which index is N, if not be used,
 * reserved it, or else continue to check next, until get valid chunks,
 * if not, get a new block.
 *
 * the memory in slice0 and slice1 will do a simple balance by used size,
 * when the memory size which is allocated from slice0 is greater than
 * slice1, then next time memory will be allocated from slice1; and the
 * allocation from slice0 is the other way round.
 */

#include <linux/module.h>
#include <linux/miscdevice.h>
#include <linux/pci.h>
#include <linux/pagemap.h>

#include "common.h"
#include "memory.h"
#include "transcoder.h"

/* aggress as follows: the first slice is slice_0, another is slice_1 */
#define MAX_TASK_NUM 16
#define MIN_TASK_ID 1
#define S0_BLOCK_CNT 8
#define S1_BLOCK_CNT 8
#define CHUNK_SIZE 0x1000

#define SLICE0_INDEX 0
#define SLICE1_INDEX 1

/* exclude tcache,first 0x200000 for edma link table.*/
unsigned long s0_start = 0x4200000;
/* exclude ecc data */
unsigned long s0_end = 0x70000000;
/* exclude tcache */
unsigned long s1_start = 0x84000000;
/* exclude ecc data */
unsigned long s1_end = 0xF0000000;

module_param(s0_start, ulong, 0644);
MODULE_PARM_DESC(s0_start, "ddr0 start address; default is 0x4200000.");

module_param(s0_end, ulong, 0644);
MODULE_PARM_DESC(s0_end, "ddr0 end address; default is 0x70000000.");

module_param(s1_start, ulong, 0644);
MODULE_PARM_DESC(s1_start, "ddr1 start address; default is 0x84000000.");

module_param(s1_end, ulong, 0644);
MODULE_PARM_DESC(s1_end, "ddr1 end address; default is 0xF0000000.");

/* 0:use slice_0 1:use_slice_1 2:balance s0 and s1 */
unsigned int method = 2;

module_param(method, uint, 0644);
MODULE_PARM_DESC(method, "allocate mem method,0:use s0,1:use s1,2:balance "
                         "s0/s1;default is 2");

/* taskid status */
#define ID_BUSY 0x1
#define ID_FREE 0x0

/* memory chunk information */
struct hlina_chunk {
    u64 bus_add; /* this chunk start physical address */
    u32 ck_rsv; /* indicate this chunk whether be used or not */
};

/* memory block information */
struct mem_block {
    u32 index; /* block index */
    u64 start_addr; /*  the block start address; */
    int task_id; /* this block be used by task_id */
    u32 block_size; /* block size; */
    u32 chunks; /* total chunk count */
    u32 free_chunks; /* the chunks not used */
    struct hlina_chunk *chk_inf;
};

/* The memory_t structure describes memory module */
struct memory_t {
    struct cb_tranx_t *tdev;
    struct mutex mem_mutex_ep;
    struct mutex mem_mutex_rc;

    struct mem_block *mem_s0_bk; /* memory slice0 block*/
    struct mem_block *mem_s1_bk; /* memory slice1 block*/
    u32 s0_rev_size; /* slice0_reserved_size */
    u32 s1_rev_size; /* slice1_reserved_size */

    spinlock_t taskid_lock;
    int id_st[MAX_TASK_NUM + 1]; /* task id status */
    struct file *id_filp[MAX_TASK_NUM + 1]; /* save owner who use the id */
};

/* get a not used task id, maximun support 32 task, so total count of
 * task id is 32, from 1 to 32.
 */
static int get_task_id(struct memory_t *tmem, struct file *filp)
{
    int i;

    spin_lock(&tmem->taskid_lock);
    for (i = MIN_TASK_ID; i <= MAX_TASK_NUM; i++) {
        if (tmem->id_st[i] == ID_FREE) {
            tmem->id_st[i]   = ID_BUSY;
            tmem->id_filp[i] = filp;
            break;
        }
    }
    spin_unlock(&tmem->taskid_lock);

    return (i <= MAX_TASK_NUM) ? i : -1;
}

/* if free a task id, will free all corresponding memory. */
static void free_task_id(struct memory_t *tmem, int id)
{
    int i, j;
    struct mem_block *sbk;
    int mem_leak_flag_0 = 0;
    int mem_leak_flag_1 = 0;

    spin_lock(&tmem->taskid_lock);
    tmem->id_st[id]   = ID_FREE;
    tmem->id_filp[id] = NULL;

    for (i = 0; i < S0_BLOCK_CNT; i++) {
        if (tmem->mem_s0_bk[i].task_id == id) {
            sbk = &(tmem->mem_s0_bk[i]);
            for (j = 0; j < sbk->chunks; j++) {
                if (sbk->chk_inf[j].ck_rsv) {
                    mem_leak_flag_0++;
                    tmem->s0_rev_size -= sbk->chk_inf[j].ck_rsv * CHUNK_SIZE;
                    sbk->chk_inf[j].ck_rsv = 0;
                    trans_dbg(tmem->tdev, TR_DBG,
                              "mem: mem leak,s0,blk:%d,id:%d,add=0x%llx\n", i,
                              id, sbk->chk_inf[j].bus_add);
                }
            }
            sbk->task_id = 0;
        }
    }
    for (i = 0; i < S1_BLOCK_CNT; i++) {
        if (tmem->mem_s1_bk[i].task_id == id) {
            sbk = &(tmem->mem_s1_bk[i]);
            for (j = 0; j < sbk->chunks; j++) {
                if (sbk->chk_inf[j].ck_rsv) {
                    mem_leak_flag_1++;
                    tmem->s1_rev_size -= sbk->chk_inf[j].ck_rsv * CHUNK_SIZE;
                    sbk->chk_inf[j].ck_rsv = 0;
                    trans_dbg(tmem->tdev, TR_DBG,
                              "mem: mem leak,s1,blk:%d,id:%d,add=0x%llx\n", i,
                              id, sbk->chk_inf[j].bus_add);
                }
            }
            sbk->task_id = 0;
        }
    }
    if (mem_leak_flag_0)
        trans_dbg(tmem->tdev, TR_ERR, "mem: mem leak in slice_0\n");
    if (mem_leak_flag_1)
        trans_dbg(tmem->tdev, TR_ERR, "mem: mem leak in slice_1\n");

    spin_unlock(&tmem->taskid_lock);
}

void cb_mem_close(struct cb_tranx_t *tdev, struct file *filp)
{
    int id;
    struct memory_t *tmem = tdev->modules[TR_MODULE_MEMORY];

    for (id = MIN_TASK_ID; id <= MAX_TASK_NUM; id++) {
        if (tmem->id_filp[id] == filp) {
            trans_dbg(tdev, TR_ERR, "%s id:%d\n", __func__, id);
            free_task_id(tmem, id);
            break;
        }
    }
}

/* Cycle through the buffers we have, give the first free one */
static int get_chunks(int slice, int block_index, unsigned long *busaddr,
                      unsigned int size, int task_id, struct memory_t *tmem)
{
    int i, j;
    int ret = 0;
    int bi  = block_index;
    struct mem_block *mb;
    unsigned int skip_chunks = 0;
    /* calculate how many chunks we need;round up to chunk boundary */
    unsigned int alloc_chunks = (size + CHUNK_SIZE - 1) / CHUNK_SIZE;

    if (slice > 1) {
        trans_dbg(tmem->tdev, TR_ERR, "mem: %s, slice id:%d is error\n",
                  __func__, slice);
        return -EFAULT;
    }

    if (slice == 0)
        mb = tmem->mem_s0_bk;
    else
        mb = tmem->mem_s1_bk;

    *busaddr = 0;
    /* run through the chunk table */
    for (i = 0; i < mb[bi].chunks;) {
        skip_chunks = 0;
        /* if this chunk is available */
        if (!mb[bi].chk_inf[i].ck_rsv) {
            /* check that there is enough memory left */
            if (i + alloc_chunks > mb[bi].chunks)
                break;

            /*
             * check that there is enough consecutive
             * chunks available.
             */
            for (j = i; j < i + alloc_chunks; j++) {
                if (mb[bi].chk_inf[j].ck_rsv) {
                    skip_chunks = 1;
                    /* skip the used chunks */
                    i = j + mb[bi].chk_inf[j].ck_rsv;
                    break;
                }
            }

            /* if enough free memory found */
            if (!skip_chunks) {
                *busaddr                 = mb[bi].chk_inf[i].bus_add;
                mb[bi].chk_inf[i].ck_rsv = alloc_chunks;
                mb[bi].free_chunks -= alloc_chunks;

                if (slice == 0)
                    tmem->s0_rev_size += alloc_chunks * CHUNK_SIZE;
                else
                    tmem->s1_rev_size += alloc_chunks * CHUNK_SIZE;

                break;
            }
        } else {
            /* skip the used chunks */
            i += mb[bi].chk_inf[i].ck_rsv;
        }
    }

    if (*busaddr == 0) {
        trans_dbg(tmem->tdev, TR_DBG,
                  "mem: get chunks failed in s:%d,block:%d,size:0x%x\n", slice,
                  bi, size);
        ret = -EFAULT;
    } else
        trans_dbg(tmem->tdev, TR_DBG,
                  "mem: alloc ep s:%d block:%d,addr:0x%lx "
                  "size:0x%x,task_id:%d\n",
                  slice, bi, *busaddr, size, task_id);

    return ret;
}

/* allocate memory from a new block */
int alloc_in_new_block(int slice, unsigned long *addr, unsigned int size,
                       int task_id, struct memory_t *tmem)
{
    int i, j, blk_cnt;
    int ret = 0;
    struct mem_block *mem_block;

    if (slice == SLICE0_INDEX) {
        mem_block = tmem->mem_s0_bk;
        blk_cnt   = S0_BLOCK_CNT;
    } else if (slice == SLICE1_INDEX) {
        mem_block = tmem->mem_s1_bk;
        blk_cnt   = S1_BLOCK_CNT;
    } else {
        trans_dbg(tmem->tdev, TR_ERR, "mem: %s, slice id:%d is error\n",
                  __func__, slice);
        return -EFAULT;
    }

    *addr = 0;
    /* traversal all blocks, get a new block. */
    for (i = 0; i < blk_cnt; i++) {
        if (mem_block[i].task_id == 0) {
            ret = get_chunks(slice, i, addr, size, task_id, tmem);
            if (ret) {
                trans_dbg(tmem->tdev, TR_ERR,
                          "mem: the size:0x%x is very large, more than "
                          "slice:%d block:%d size, task_id=%d\n",
                          size, slice, i, task_id);
                for (j = 0; j < mem_block[i].chunks; j++) {
                    if (mem_block[i].chk_inf[j].ck_rsv)
                        trans_dbg(tmem->tdev, TR_ERR,
                                  "mem: j:%d reserve_cnt:%d\n", j,
                                  mem_block[i].chk_inf[j].ck_rsv);
                }
            } else {
                mem_block[i].task_id = task_id;
                break;
            }
        }
    }

    /*  not has valid block, alloc failed. */
    if (i == blk_cnt) {
        trans_dbg(tmem->tdev, TR_DBG,
                  "mem: %s, not has valid block in slice%d.\n", __func__,
                  slice);
        return -EFAULT;
    }

    return ret;
}

/*
 * get memory from a slice.
 * @s: slice id.
 * @id: task id
 * @check: check:1, only get memory from used block
 *         check:0, if can't get valid memory in used block, allocate from
 *                  a new block.
 */
static int alloc_mem_in_slice(int s, unsigned long *addr, unsigned int size,
                              int id, struct memory_t *tmem, int check)
{
    int i, blk_cnt;
    int ret = 0;
    struct mem_block *mem_block;

    if (s == SLICE0_INDEX) {
        mem_block = tmem->mem_s0_bk;
        blk_cnt   = S0_BLOCK_CNT;
    } else if (s == SLICE1_INDEX) {
        mem_block = tmem->mem_s1_bk;
        blk_cnt   = S1_BLOCK_CNT;
    } else {
        trans_dbg(tmem->tdev, TR_ERR,
                  "mem: slice id:%d is error in alloc mem in slice\n", s);
        return -EFAULT;
    }
    *addr = 0;

    /* traversal all blocks,check itself block if it has valid memory */
    for (i = 0; i < blk_cnt; i++) {
        if (mem_block[i].task_id == id) {
            ret = get_chunks(s, i, addr, size, id, tmem);
            if (!ret)
                break;
        }
    }

    /* not has valid memory in itself block */
    if (i == blk_cnt) {
        if (check == 1)
            ret = -EFAULT;
        else
            /* get a new block; */
            ret = alloc_in_new_block(s, addr, size, id, tmem);
    }

    return ret;
}

/* allocate memory from its own used block */
static int alloc_in_used_block(unsigned long *busaddr, unsigned int size,
                               int task_id, struct memory_t *tmem)
{
    int ret;

    if (tmem->s0_rev_size <= tmem->s1_rev_size) {
        /* check slice0 if there is valid used block */
        ret = alloc_mem_in_slice(SLICE0_INDEX, busaddr, size, task_id, tmem, 1);
        /* s0 not has valid used blocks for this id, check s1. */
        if (ret) {
            /* check slice1 if it has valid used block. */
            ret = alloc_mem_in_slice(SLICE1_INDEX, busaddr, size, task_id, tmem,
                                     1);
        }
    } else {
        /* check slice1 if it has valid used block */
        ret = alloc_mem_in_slice(SLICE1_INDEX, busaddr, size, task_id, tmem, 1);
        /* s1 not has valid used blocks for this id, check s0. */
        if (ret) {
            /* check s0 if it has valid used block. */
            ret = alloc_mem_in_slice(SLICE0_INDEX, busaddr, size, task_id, tmem,
                                     1);
        }
    }

    return ret;
}

/*
 * allocate memory in ep side.
 * method is the allocation strategy.
 *	0: only use slice_0
 *	1: only use_slice_1
 *	2: balance s0 and s1 by used size.
 */
static int alloc_mem_ep(unsigned long *busaddr, unsigned int size, int task_id,
                        struct memory_t *tmem)
{
    int ret  = 0;
    *busaddr = 0;

    if (WARN_ON(method > 2)) {
        trans_dbg(tmem->tdev, TR_ERR, "mem: %s, method:%d error\n", __func__);
        return -EFAULT;
    }

    /* alloc memory from slice 0  */
    if (method == 0)
        ret = alloc_mem_in_slice(SLICE0_INDEX, busaddr, size, task_id, tmem, 0);

    /* alloc memory from slice 1 */
    else if (method == 1)
        ret = alloc_mem_in_slice(SLICE1_INDEX, busaddr, size, task_id, tmem, 0);

    /* alloc memory from slice 0 and slice 1, a simple balance */
    else {
        if (alloc_in_used_block(busaddr, size, task_id, tmem)) {
            if (tmem->s0_rev_size <= tmem->s1_rev_size) {
                /* first alloc from s0 */
                ret = alloc_in_new_block(SLICE0_INDEX, busaddr, size, task_id,
                                         tmem);
                if (ret)
                    ret = alloc_in_new_block(SLICE1_INDEX, busaddr, size,
                                             task_id, tmem);
            } else {
                /* first alloc from s1 */
                ret = alloc_in_new_block(SLICE1_INDEX, busaddr, size, task_id,
                                         tmem);
                if (ret)
                    ret = alloc_in_new_block(SLICE0_INDEX, busaddr, size,
                                             task_id, tmem);
            }
        }
    }

    if (!*busaddr) {
        trans_dbg(tmem->tdev, TR_DBG, "mem: alloc ep mem failed\n");
        ret = -EFAULT;
    }

    return ret;
}

/*
 * when free memory, will check the block which did this memory used
 * if this block is not used, free it.
 */
static int free_mem_ep(unsigned long busaddr, unsigned int size, int task_id,
                       struct memory_t *tmem)
{
    int i, ret = 0;
    int slice, bi; /* bi:block index */
    struct mem_block *mb; /* memory block */
    unsigned long tmp;
    u32 rsv_size;

    /* Find the slice it belongs to */
    if ((busaddr >= s0_start) && (busaddr < s0_end)) {
        slice = SLICE0_INDEX;
        mb    = tmem->mem_s0_bk;

        /* Find the block it belongs to */
        for (i = 0; i < S0_BLOCK_CNT; i++) {
            tmp = mb[i].start_addr + mb[i].block_size;
            if ((busaddr >= mb[i].start_addr) && (busaddr < tmp)) {
                bi = i;
                break;
            }
        }
    } else if ((busaddr >= s1_start) && (busaddr < s1_end)) {
        slice = SLICE1_INDEX;
        mb    = tmem->mem_s1_bk;

        for (i = 0; i < S1_BLOCK_CNT; i++) {
            tmp = mb[i].start_addr + mb[i].block_size;
            if ((busaddr >= mb[i].start_addr) && (busaddr < tmp)) {
                bi = i;
                break;
            }
        }
    } else {
        trans_dbg(tmem->tdev, TR_ERR, "mem: busaddr:0x%lx is invalid\n",
                  busaddr);
        ret = -EFAULT;
        goto err;
    }

    if (mb[bi].task_id != task_id) {
        trans_dbg(tmem->tdev, TR_ERR,
                  "mem: %s Wrong task_id:%d on block:%d,expected id is %d\n",
                  __func__, task_id, bi, mb[bi].task_id);
        trans_dbg(tmem->tdev, TR_ERR,
                  "mem: %s Owner mismatch while freeing memory!\n", __func__);
        ret = -EFAULT;
        goto err;
    }

    trans_dbg(tmem->tdev, TR_DBG,
              "mem: %s, busaddr:0x%lx, size:0x%x, in slice:%d block:%d.\n",
              __func__, busaddr, size, slice, bi);

    for (i = 0; i < mb[bi].chunks; i++) {
        if (mb[bi].chk_inf[i].bus_add == busaddr) {
            rsv_size = mb[bi].chk_inf[i].ck_rsv * CHUNK_SIZE;
            if (slice == SLICE0_INDEX)
                tmem->s0_rev_size -= rsv_size;
            else
                tmem->s1_rev_size -= rsv_size;

            mb[bi].free_chunks += mb[bi].chk_inf[i].ck_rsv;
            mb[bi].chk_inf[i].ck_rsv = 0;

            if (mb[bi].free_chunks == mb[bi].chunks) {
                trans_dbg(tmem->tdev, TR_DBG,
                          "mem: %s block:%d is free in slice:%d\n", __func__,
                          bi, slice);
                mb[bi].task_id = 0;
            }
            break;
        }
    }

    if (i == mb[bi].chunks) {
        trans_dbg(tmem->tdev, TR_ERR,
                  "mem: %s can't find the chunk,busaddr:0x%lx size:0x%x\n",
                  __func__, busaddr, size);
        ret = -EFAULT;
        goto err;
    }

err:
    return ret;
}

/* Compute memory utilization */
static void mem_usage(struct cb_tranx_t *tdev, struct mem_used_info *info)
{
    int i;
    struct memory_t *tmem = tdev->modules[TR_MODULE_MEMORY];

    info->s0_blk_used = 0;
    info->s1_blk_used = 0;

    for (i = 0; i < S0_BLOCK_CNT; i++) {
        if (tmem->mem_s0_bk[i].task_id)
            info->s0_blk_used++;
    }
    for (i = 0; i < S1_BLOCK_CNT; i++) {
        if (tmem->mem_s1_bk[i].task_id)
            info->s1_blk_used++;
    }

    info->s0_used = tmem->s0_rev_size / 1024 / 1024;
    info->s0_free = (s0_end - s0_start - tmem->s0_rev_size) / 1024 / 1024;
    info->s1_used = tmem->s1_rev_size / 1024 / 1024;
    info->s1_free = (s1_end - s1_start - tmem->s1_rev_size) / 1024 / 1024;
}

/* display  memory status */
static ssize_t mem_info_show(struct device *dev, struct device_attribute *attr,
                             char *buf)
{
    int pos                 = 0;
    struct cb_tranx_t *tdev = dev_get_drvdata(dev);
    struct memory_t *tmem   = tdev->modules[TR_MODULE_MEMORY];
    struct mem_block *s0_bk = tmem->mem_s0_bk;
    struct mem_block *s1_bk = tmem->mem_s1_bk;

    pos += sprintf(buf + pos, "S0:  %d MB used   %ld MB free   %ld%% used\n",
                   tmem->s0_rev_size / 1024 / 1024,
                   (s0_end - s0_start - tmem->s0_rev_size) / 1024 / 1024,
                   tmem->s0_rev_size / ((s0_end - s0_start) / 100));
    pos += sprintf(buf + pos, "S1:  %d MB used   %ld MB free   %ld%% used\n",
                   tmem->s1_rev_size / 1024 / 1024,
                   (s1_end - s1_start - tmem->s1_rev_size) / 1024 / 1024,
                   tmem->s1_rev_size / ((s1_end - s1_start) / 100));

    pos += sprintf(buf + pos, "block:  0   1   2   3   4   5   6   7\n");
    pos += sprintf(buf + pos, "   S0: %2d  %2d  %2d  %2d  %2d  %2d  %2d  %2d\n",
                   s0_bk[0].task_id, s0_bk[1].task_id, s0_bk[2].task_id,
                   s0_bk[3].task_id, s0_bk[4].task_id, s0_bk[5].task_id,
                   s0_bk[6].task_id, s0_bk[7].task_id);
    pos += sprintf(buf + pos, "   S1: %2d  %2d  %2d  %2d  %2d  %2d  %2d  %2d\n",
                   s1_bk[0].task_id, s1_bk[1].task_id, s1_bk[2].task_id,
                   s1_bk[3].task_id, s1_bk[4].task_id, s1_bk[5].task_id,
                   s1_bk[6].task_id, s1_bk[7].task_id);

    return pos;
}

static DEVICE_ATTR_RO(mem_info);

static struct attribute *transmem_sysfs_entries[] = { &dev_attr_mem_info.attr,
                                                      NULL };

static struct attribute_group transmem_attribute_group = {
    .name  = NULL,
    .attrs = transmem_sysfs_entries,
};

int cb_mem_init(struct cb_tranx_t *tdev)
{
    int ret = -EFAULT;
    u32 blk_size, i, j;
    void *block;
    void *hlina_chunks;
    struct mem_block *mem_s0_bk;
    struct mem_block *mem_s1_bk;
    u64 tmp_addr;
    struct memory_t *tmem;

    tmem = kzalloc(sizeof(struct memory_t), GFP_KERNEL);
    if (!tmem) {
        trans_dbg(tdev, TR_ERR, "mem: alloc tmem error\n");
        goto out;
    }
    tdev->modules[TR_MODULE_MEMORY] = tmem;
    tmem->tdev                      = tdev;

    block = kzalloc(sizeof(struct mem_block) * (S0_BLOCK_CNT + S1_BLOCK_CNT),
                    GFP_KERNEL);
    if (!block) {
        trans_dbg(tdev, TR_ERR, "mem: alloc block error\n");
        goto out_free_dev;
    }

    hlina_chunks = vzalloc((s0_end - s0_start + s1_end - s1_start) /
                           CHUNK_SIZE * sizeof(struct hlina_chunk));
    if (!hlina_chunks) {
        trans_dbg(tdev, TR_ERR, "mem: alloc hlina_chunks failed\n");
        goto out_free_slice;
    }

    mem_s0_bk = block;
    blk_size  = ((s0_end - s0_start) / S0_BLOCK_CNT) & (~(CHUNK_SIZE - 1));
    trans_dbg(tdev, TR_DBG,
              "memory : s0_start:0x%08lx s0_size:0x%lx block_cnt:%d, "
              "block_size:0x%lx,chunks:%ld.\n",
              s0_start, s0_end - s0_start, S0_BLOCK_CNT, blk_size,
              blk_size * S0_BLOCK_CNT / CHUNK_SIZE);
    for (i = 0; i < S0_BLOCK_CNT; i++) {
        mem_s0_bk[i].index       = i;
        mem_s0_bk[i].task_id     = 0;
        mem_s0_bk[i].block_size  = blk_size;
        mem_s0_bk[i].start_addr  = s0_start + blk_size * i;
        mem_s0_bk[i].chunks      = mem_s0_bk[i].block_size / CHUNK_SIZE;
        mem_s0_bk[i].free_chunks = mem_s0_bk[i].chunks;
        mem_s0_bk[i].chk_inf     = hlina_chunks;
        hlina_chunks += sizeof(struct hlina_chunk) * mem_s0_bk[i].chunks;
        tmp_addr = mem_s0_bk[i].start_addr;
        for (j = 0; j < mem_s0_bk[i].chunks; j++) {
            mem_s0_bk[i].chk_inf[j].bus_add = tmp_addr;
            mem_s0_bk[i].chk_inf[j].ck_rsv  = 0;
            tmp_addr += CHUNK_SIZE;
        }
    }

    mem_s1_bk = block + sizeof(struct mem_block) * S0_BLOCK_CNT;
    blk_size  = ((s1_end - s1_start) / S1_BLOCK_CNT) & (~(CHUNK_SIZE - 1));
    trans_dbg(tdev, TR_DBG,
              "memory : s1_start:0x%08lx s1_size:0x%lx block_cnt:%d, "
              "block_size:0x%lx,chunks:%ld.\n",
              s1_start, s1_end - s1_start, S1_BLOCK_CNT, blk_size,
              blk_size * S0_BLOCK_CNT / CHUNK_SIZE);
    for (i = 0; i < S1_BLOCK_CNT; i++) {
        mem_s1_bk[i].index       = i;
        mem_s1_bk[i].task_id     = 0;
        mem_s1_bk[i].block_size  = blk_size;
        mem_s1_bk[i].start_addr  = s1_start + blk_size * i;
        mem_s1_bk[i].chunks      = mem_s1_bk[i].block_size / CHUNK_SIZE;
        mem_s1_bk[i].free_chunks = mem_s1_bk[i].chunks;
        mem_s1_bk[i].chk_inf     = hlina_chunks;
        hlina_chunks += sizeof(struct hlina_chunk) * mem_s1_bk[i].chunks;
        tmp_addr = mem_s1_bk[i].start_addr;
        for (j = 0; j < mem_s1_bk[i].chunks; j++) {
            mem_s1_bk[i].chk_inf[j].bus_add = tmp_addr;
            mem_s1_bk[i].chk_inf[j].ck_rsv  = 0;
            tmp_addr += CHUNK_SIZE;
        }
    }

    tmem->mem_s0_bk   = mem_s0_bk;
    tmem->mem_s1_bk   = mem_s1_bk;
    tmem->s0_rev_size = 0;
    tmem->s1_rev_size = 0;

    mutex_init(&tmem->mem_mutex_ep);
    mutex_init(&tmem->mem_mutex_rc);
    spin_lock_init(&tmem->taskid_lock);

    ret = sysfs_create_group(&tdev->misc_dev->this_device->kobj,
                             &transmem_attribute_group);
    if (ret) {
        trans_dbg(tdev, TR_ERR, "mem: failed to create sysfs entry\n");
        goto out_free_chunk;
    }

    trans_dbg(tdev, TR_INF, "mem: module initialize done.\n");
    return 0;

out_free_chunk:
    vfree(hlina_chunks);
out_free_slice:
    kfree(block);
out_free_dev:
    kfree(tmem);
out:
    trans_dbg(tdev, TR_ERR, "mem: module initialize failed.\n");
    return ret;
}

int cb_mem_release(struct cb_tranx_t *tdev)
{
    struct memory_t *tmem = tdev->modules[TR_MODULE_MEMORY];

    mutex_destroy(&tmem->mem_mutex_ep);
    mutex_destroy(&tmem->mem_mutex_rc);

    vfree(tmem->mem_s0_bk[0].chk_inf);
    kfree(tmem->mem_s0_bk);

    sysfs_remove_group(&tdev->misc_dev->this_device->kobj,
                       &transmem_attribute_group);

    kfree(tmem);
    trans_dbg(tdev, TR_DBG, "mem: remove module done.\n");

    return 0;
}

static int check_task_id(struct memory_t *tmem, int task_id)
{
    if ((task_id > MAX_TASK_NUM) || (task_id < MIN_TASK_ID) ||
        (tmem->id_st[task_id] == ID_FREE)) {
        return -EFAULT;
    } else
        return 0;
}
long cb_mem_ioctl(struct file *filp, unsigned int cmd, unsigned long arg,
                  struct cb_tranx_t *tdev)
{
    int ret = 0;
    int id;
    struct mem_used_info info;
    struct mem_info memp;
    unsigned long addr;
    void __user *argp     = (void __user *)arg;
    struct memory_t *tmem = tdev->modules[TR_MODULE_MEMORY];

    switch (cmd) {
    case CB_TRANX_MEM_GET_UTIL:
        mem_usage(tdev, &info);
        if (__copy_to_user(argp, &info, sizeof(info))) {
            trans_dbg(tdev, TR_ERR,
                      "mem: get mem usage - copy_to_user failed.\n");
            ret = -EFAULT;
        }
        break;
    case CB_TRANX_MEM_ALLOC:
        if (copy_from_user(&memp, argp, sizeof(memp))) {
            trans_dbg(tdev, TR_ERR, "mem: get mem - copy_from_user failed.\n");
            return -EFAULT;
        }
        if (memp.mem_location) {
            trans_dbg(tdev, TR_ERR, "mem: get mem - mem_location error.\n");
            return -EFAULT;
        }
        if (check_task_id(tmem, memp.task_id)) {
            trans_dbg(tdev, TR_ERR, "mem: get mem, id:%d is error\n",
                      memp.task_id);
            return -EFAULT;
        }

        if (mutex_lock_interruptible(&tmem->mem_mutex_ep))
            return -EFAULT;
        ret = alloc_mem_ep(&addr, memp.size, memp.task_id, tmem);
        mutex_unlock(&tmem->mem_mutex_ep);

        if (ret) {
            trans_dbg(tdev, TR_ERR, "mem: alloc memory failed.\n");
            return -EFAULT;
        }
        memp.phy_addr = addr;
        if (__copy_to_user(argp, &memp, sizeof(memp))) {
            trans_dbg(tdev, TR_ERR,
                      "mem: get mem, copy_to_user failed, then free it\n");
            free_mem_ep(addr, memp.size, memp.task_id, tmem);
            return -EFAULT;
        }

        break;
    case CB_TRANX_MEM_FREE:
        if (copy_from_user(&memp, argp, sizeof(memp))) {
            trans_dbg(tdev, TR_ERR, "mem: free mem - copy_from_user failed.\n");
            return -EFAULT;
        }
        if (memp.mem_location) {
            trans_dbg(tdev, TR_ERR, "mem: free mem - mem_location error.\n");
            return -EFAULT;
        }
        if (check_task_id(tmem, memp.task_id)) {
            trans_dbg(tdev, TR_ERR, "mem: free mem,id:%d is error\n",
                      memp.task_id);
            return -EFAULT;
        }

        if (mutex_lock_interruptible(&tmem->mem_mutex_ep))
            return -EFAULT;
        ret = free_mem_ep(memp.phy_addr, memp.size, memp.task_id, tmem);
        mutex_unlock(&tmem->mem_mutex_ep);

        if (ret) {
            trans_dbg(tdev, TR_ERR, "mem: free memory failed\n");
            return -EFAULT;
        }
        break;
    case CB_TRANX_MEM_GET_TASKID:
        id = get_task_id(tmem, filp);
        if (id == -1) {
            trans_dbg(tdev, TR_ERR, "mem: can't get valid id\n");
            return -EFAULT;
        }
        __put_user(id, (int *)argp);
        break;
    case CB_TRANX_MEM_FREE_TASKID:
        __get_user(id, (int *)argp);
        if (check_task_id(tmem, id)) {
            trans_dbg(tdev, TR_ERR, "mem: free id, id:%d is error\n", id);
            return -EFAULT;
        }
        free_task_id(tmem, id);
        break;
    default:
        trans_dbg(tdev, TR_ERR, "mem: %s, cmd:0x%x is error.\n", __func__, cmd);
        ret = -EINVAL;
    }

    return ret;
}
